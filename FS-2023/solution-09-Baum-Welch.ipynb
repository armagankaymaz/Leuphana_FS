{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"exercise-02-data.npz\")\n",
    "x = data[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(1000, 5)\n",
      "x[0]=array([1, 0, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x.shape=}\")\n",
    "print(f\"{x[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where:\n",
    "\n",
    "WALK = 0\n",
    "SHOP = 1\n",
    "CLEAN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# quick one hot encoding\n",
    "N, T, K, D = 1000, 5, 2, 3\n",
    "\n",
    "x_one_hot = np.zeros((N, T, D))\n",
    "x_one_hot[..., 0] = x == 0\n",
    "x_one_hot[..., 1] = x == 1\n",
    "x_one_hot[..., 2] = x == 2\n",
    "\n",
    "assert x_one_hot.sum() == N * T\n",
    "\n",
    "print(f\"{x_one_hot[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying over `forward_backward_v` from Exercise 2, changing it to take log_pi, log_a, log_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_v(x, log_pi, log_a, log_b, debug=True):\n",
    "    from scipy.special import logsumexp\n",
    "\n",
    "    num_states = log_pi.size\n",
    "    seq_len = len(x)\n",
    "\n",
    "    log_alpha = np.zeros((seq_len, num_states))\n",
    "    log_beta = np.zeros((seq_len, num_states))\n",
    "\n",
    "    # calculate alpha\n",
    "    log_alpha[0] = log_pi + log_b[:, x[0]]\n",
    "    for t in range(seq_len-1):\n",
    "        log_alpha[t+1] = logsumexp(log_alpha[t, :].reshape(-1, 1) + log_a[:, :] + log_b[:, x[t+1]].reshape(1, -1), axis=0)\n",
    "\n",
    "    # calculate beta\n",
    "    log_beta[-1] = 0  # redundant, due to initialization above 0 = log(1)\n",
    "    for t in range(seq_len-2,-1,-1):\n",
    "        log_beta[t] = logsumexp(log_a[:, :] + log_b[:, x[t+1]].reshape(1, -1) + log_beta[t+1, :], axis=1)\n",
    "    \n",
    "    # calculate log_gamma (unnormalized)\n",
    "    log_gamma_unnormalized = log_alpha + log_beta\n",
    "\n",
    "    if debug == True:\n",
    "        # calculate log_p_x_lambda (and check if values are as expected)\n",
    "        log_p_x_lambda = np.zeros(seq_len)\n",
    "        for t in range(seq_len):\n",
    "            log_p_x_lambda[t] = logsumexp(log_alpha[t] + log_beta[t])\n",
    "        \n",
    "        assert np.allclose(log_p_x_lambda, log_p_x_lambda[0])\n",
    "        try:\n",
    "            assert 0 <= np.exp(log_p_x_lambda[0]) <= 1\n",
    "        except:\n",
    "            raise ValueError(f\"{log_p_x_lambda=}\")\n",
    "\n",
    "        gamma = np.exp(log_gamma_unnormalized - log_p_x_lambda.reshape(-1, 1))\n",
    "        try:\n",
    "            assert (0 <= gamma).all() and (gamma <= 1).all()\n",
    "        except:\n",
    "            raise ValueError(f\"gamma:\\n{gamma}\")\n",
    "    else:\n",
    "        log_p_x_lambda = None\n",
    "    \n",
    "    # get argmax over gammas\n",
    "    y = np.argmax(np.exp(log_gamma_unnormalized), axis=1)\n",
    "        \n",
    "    return y, log_alpha, log_beta, log_p_x_lambda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Baum-Welch (not optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy parameters for testing\n",
    "\n",
    "num_states = 2\n",
    "num_observations = 3\n",
    "\n",
    "pi_est = np.ones((num_states)) / num_states\n",
    "a_est = np.ones((num_states, num_states)) / num_states\n",
    "b_est = np.ones((num_states, num_observations)) / num_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_est=array([0.5, 0.5])\n",
      "a_est=array([[0.5, 0.5],\n",
      "       [0.5, 0.5]])\n",
      "b_est=array([[0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pi_est=}\")\n",
    "print(f\"{a_est=}\")\n",
    "print(f\"{b_est=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi_est, log_a_est, log_b_est = np.log(pi_est), np.log(a_est), np.log(b_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(x, log_pi, log_a, log_b):\n",
    "    N, T = x.shape\n",
    "    K, K_ = log_a.shape\n",
    "    assert K == K_\n",
    "    K_, D = log_b.shape\n",
    "    assert K == K_\n",
    "\n",
    "    y = -1 * np.ones((N, T))\n",
    "    log_alpha = -1 * np.ones((N, T, K))\n",
    "    log_beta = -1 * np.ones((N, T, K))\n",
    "    log_p_x_lambda = -1 * np.ones((N, T))  # output is duplicated T times\n",
    "\n",
    "    for idx, seq in enumerate(x):\n",
    "        out = forward_backward_v(x=seq, log_pi=log_pi, log_a=log_a, log_b=log_b, debug=True)\n",
    "        y[idx], log_alpha[idx], log_beta[idx], log_p_x_lambda[idx] = out\n",
    "    \n",
    "    return y, log_alpha, log_beta, log_p_x_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing e_step\n",
    "\n",
    "_, log_alpha, log_beta, log_p_x_lambda = e_step(x=x, log_pi=log_pi_est, log_a=log_a_est, log_b=log_b_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(x, x_one_hot, log_a, log_b, log_alpha, log_beta, log_p_x_lambda):\n",
    "    N, T, K = log_alpha.shape\n",
    "    log_gamma = log_alpha + log_beta - log_p_x_lambda.reshape(N, T, 1)\n",
    "\n",
    "    # max pi\n",
    "    log_pi_new = logsumexp(log_gamma[:, 0, :], axis=0) - np.log(N)\n",
    "    assert np.allclose(np.exp(logsumexp(log_pi_new)), 1)\n",
    "\n",
    "    # max a\n",
    "    log_a_new = np.ones_like(log_a) * -1\n",
    "    for j in range(K):\n",
    "        for k in range(K):\n",
    "            log_xi_jk = np.ones((N, T-1)) * -1\n",
    "            for n in range(N):\n",
    "                for t in range(T-1):\n",
    "                    log_xi_jk[n, t] = log_alpha[n, t, j] + log_a[j, k] + log_b[k, x[n, t+1]] + log_beta[n, t+1, k] - log_p_x_lambda[n, 0]\n",
    "            log_a_new[j, k] = logsumexp(log_xi_jk) - logsumexp(log_gamma[:, :-1, j])\n",
    "\n",
    "    # max b\n",
    "    log_b_new = np.ones_like(log_b) * -1\n",
    "    for i in range(D):\n",
    "        log_b_new[..., i] = logsumexp(log_gamma + np.expand_dims(np.log(x_one_hot[..., i]), axis=-1), axis=(0, 1)) - logsumexp(log_gamma, axis=(0, 1))\n",
    "\n",
    "    return log_pi_new, log_a_new, log_b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5557/4265165530.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  log_b_new[..., i] = logsumexp(log_gamma + np.expand_dims(np.log(x_one_hot[..., i]), axis=-1), axis=(0, 1)) - logsumexp(log_gamma, axis=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "# testing m_step\n",
    "\n",
    "log_pi_new, log_a_new, log_b_new = m_step(x=x, x_one_hot=x_one_hot, log_a=log_a_est, log_b=log_b_est, log_alpha=log_alpha, log_beta=log_beta, log_p_x_lambda=log_p_x_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_est=array([0.5, 0.5])\n",
      "a_est=array([[0.46325626, 0.53674374],\n",
      "       [0.51347526, 0.48652474]])\n",
      "b_est=array([[0.3071543 , 0.38105828, 0.31178742],\n",
      "       [0.37224356, 0.3914712 , 0.23628524]])\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters (with fixed random.seed ...)\n",
    "\n",
    "num_states = 2\n",
    "num_observations = 3\n",
    "\n",
    "pi_est = np.ones((num_states)) / num_states\n",
    "np.random.seed(0)\n",
    "a_est = np.random.uniform(low=0.25, high=0.75, size=(num_states, num_states))\n",
    "b_est = np.random.uniform(low=0.25, high=0.75, size=(num_states, num_observations))\n",
    "a_est = a_est / a_est.sum(axis=-1, keepdims=True)\n",
    "b_est = b_est / b_est.sum(axis=-1, keepdims=True)\n",
    "print(f\"{pi_est=}\")\n",
    "print(f\"{a_est=}\")\n",
    "print(f\"{b_est=}\")\n",
    "log_pi_est, log_a_est, log_b_est = np.log(pi_est), np.log(a_est), np.log(b_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5557/4265165530.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  log_b_new[..., i] = logsumexp(log_gamma + np.expand_dims(np.log(x_one_hot[..., i]), axis=-1), axis=(0, 1)) - logsumexp(log_gamma, axis=(0, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00000: log_p_X_lambda=-5475.4594027374205\n",
      "epoch=00001: log_p_X_lambda=-5438.252450623976\n",
      "epoch=00002: log_p_X_lambda=-5438.23565193709\n",
      "epoch=00003: log_p_X_lambda=-5438.219753782536\n",
      "epoch=00004: log_p_X_lambda=-5438.204678805732\n",
      "epoch=00005: log_p_X_lambda=-5438.1903560875935\n",
      "epoch=00006: log_p_X_lambda=-5438.176720459356\n",
      "epoch=00007: log_p_X_lambda=-5438.163711899495\n",
      "epoch=00008: log_p_X_lambda=-5438.151275002316\n",
      "epoch=00009: log_p_X_lambda=-5438.139358507848\n",
      "epoch=00010: log_p_X_lambda=-5438.127914885402\n",
      "epoch=00011: log_p_X_lambda=-5438.116899963719\n",
      "epoch=00012: log_p_X_lambda=-5438.1062726015\n",
      "epoch=00013: log_p_X_lambda=-5438.095994393847\n",
      "epoch=00014: log_p_X_lambda=-5438.0860294097565\n",
      "epoch=00015: log_p_X_lambda=-5438.076343956816\n",
      "epoch=00016: log_p_X_lambda=-5438.066906370639\n",
      "epoch=00017: log_p_X_lambda=-5438.057686825122\n",
      "epoch=00018: log_p_X_lambda=-5438.048657162162\n",
      "epoch=00019: log_p_X_lambda=-5438.039790737895\n",
      "epoch=00020: log_p_X_lambda=-5438.031062284081\n",
      "epoch=00021: log_p_X_lambda=-5438.022447783015\n",
      "epoch=00022: log_p_X_lambda=-5438.0139243543235\n",
      "epoch=00023: log_p_X_lambda=-5438.005470152777\n",
      "epoch=00024: log_p_X_lambda=-5437.99706427548\n",
      "epoch=00025: log_p_X_lambda=-5437.988686678544\n",
      "epoch=00026: log_p_X_lambda=-5437.980318100994\n",
      "epoch=00027: log_p_X_lambda=-5437.971939996516\n",
      "epoch=00028: log_p_X_lambda=-5437.963534471835\n",
      "epoch=00029: log_p_X_lambda=-5437.955084230929\n",
      "epoch=00030: log_p_X_lambda=-5437.946572524896\n",
      "epoch=00031: log_p_X_lambda=-5437.937983107553\n",
      "epoch=00032: log_p_X_lambda=-5437.929300195063\n",
      "epoch=00033: log_p_X_lambda=-5437.920508430772\n",
      "epoch=00034: log_p_X_lambda=-5437.911592853721\n",
      "epoch=00035: log_p_X_lambda=-5437.902538871498\n",
      "epoch=00036: log_p_X_lambda=-5437.89333223653\n",
      "epoch=00037: log_p_X_lambda=-5437.883959026069\n",
      "epoch=00038: log_p_X_lambda=-5437.874405625456\n",
      "epoch=00039: log_p_X_lambda=-5437.864658714412\n",
      "epoch=00040: log_p_X_lambda=-5437.854705256608\n",
      "epoch=00041: log_p_X_lambda=-5437.844532491796\n",
      "epoch=00042: log_p_X_lambda=-5437.834127930804\n",
      "epoch=00043: log_p_X_lambda=-5437.823479353277\n",
      "epoch=00044: log_p_X_lambda=-5437.812574807549\n",
      "epoch=00045: log_p_X_lambda=-5437.801402613129\n",
      "epoch=00046: log_p_X_lambda=-5437.789951365166\n",
      "epoch=00047: log_p_X_lambda=-5437.778209941213\n",
      "epoch=00048: log_p_X_lambda=-5437.766167509595\n",
      "epoch=00049: log_p_X_lambda=-5437.753813539893\n",
      "epoch=00050: log_p_X_lambda=-5437.741137814613\n",
      "epoch=00051: log_p_X_lambda=-5437.728130442469\n",
      "epoch=00052: log_p_X_lambda=-5437.714781872856\n",
      "epoch=00053: log_p_X_lambda=-5437.701082911154\n",
      "epoch=00054: log_p_X_lambda=-5437.6870247349025\n",
      "epoch=00055: log_p_X_lambda=-5437.672598910176\n",
      "epoch=00056: log_p_X_lambda=-5437.657797408385\n",
      "epoch=00057: log_p_X_lambda=-5437.642612622772\n",
      "epoch=00058: log_p_X_lambda=-5437.62703738432\n",
      "epoch=00059: log_p_X_lambda=-5437.611064977076\n",
      "epoch=00060: log_p_X_lambda=-5437.594689151855\n",
      "epoch=00061: log_p_X_lambda=-5437.5779041384885\n",
      "epoch=00062: log_p_X_lambda=-5437.560704655994\n",
      "epoch=00063: log_p_X_lambda=-5437.543085919984\n",
      "epoch=00064: log_p_X_lambda=-5437.525043647103\n",
      "epoch=00065: log_p_X_lambda=-5437.506574055926\n",
      "epoch=00066: log_p_X_lambda=-5437.487673863791\n",
      "epoch=00067: log_p_X_lambda=-5437.4683402787705\n",
      "epoch=00068: log_p_X_lambda=-5437.448570987022\n",
      "epoch=00069: log_p_X_lambda=-5437.428364133897\n",
      "epoch=00070: log_p_X_lambda=-5437.407718299106\n",
      "epoch=00071: log_p_X_lambda=-5437.386632465123\n",
      "epoch=00072: log_p_X_lambda=-5437.365105978395\n",
      "epoch=00073: log_p_X_lambda=-5437.3431385024\n",
      "epoch=00074: log_p_X_lambda=-5437.320729963055\n",
      "epoch=00075: log_p_X_lambda=-5437.29788048493\n",
      "epoch=00076: log_p_X_lambda=-5437.274590318619\n",
      "epoch=00077: log_p_X_lambda=-5437.250859758493\n",
      "epoch=00078: log_p_X_lambda=-5437.226689050715\n",
      "epoch=00079: log_p_X_lambda=-5437.202078291018\n",
      "epoch=00080: log_p_X_lambda=-5437.177027312277\n",
      "epoch=00081: log_p_X_lambda=-5437.151535561225\n",
      "epoch=00082: log_p_X_lambda=-5437.125601964539\n",
      "epoch=00083: log_p_X_lambda=-5437.0992247841605\n",
      "epoch=00084: log_p_X_lambda=-5437.072401461166\n",
      "epoch=00085: log_p_X_lambda=-5437.045128449037\n",
      "epoch=00086: log_p_X_lambda=-5437.01740103516\n",
      "epoch=00087: log_p_X_lambda=-5436.989213151707\n",
      "epoch=00088: log_p_X_lambda=-5436.960557174572\n",
      "epoch=00089: log_p_X_lambda=-5436.931423711107\n",
      "epoch=00090: log_p_X_lambda=-5436.901801376018\n",
      "epoch=00091: log_p_X_lambda=-5436.871676555411\n",
      "epoch=00092: log_p_X_lambda=-5436.841033158579\n",
      "epoch=00093: log_p_X_lambda=-5436.809852356687\n",
      "epoch=00094: log_p_X_lambda=-5436.778112308382\n",
      "epoch=00095: log_p_X_lambda=-5436.745787870768\n",
      "epoch=00096: log_p_X_lambda=-5436.712850295319\n",
      "epoch=00097: log_p_X_lambda=-5436.679266907133\n",
      "epoch=00098: log_p_X_lambda=-5436.645000765679\n",
      "epoch=00099: log_p_X_lambda=-5436.610010305953\n",
      "epoch=00100: log_p_X_lambda=-5436.574248956819\n",
      "epoch=00101: log_p_X_lambda=-5436.5376647347075\n",
      "epoch=00102: log_p_X_lambda=-5436.500199808992\n",
      "epoch=00103: log_p_X_lambda=-5436.46179003578\n",
      "epoch=00104: log_p_X_lambda=-5436.4223644563845\n",
      "epoch=00105: log_p_X_lambda=-5436.381844754957\n",
      "epoch=00106: log_p_X_lambda=-5436.340144671059\n",
      "epoch=00107: log_p_X_lambda=-5436.297169360783\n",
      "epoch=00108: log_p_X_lambda=-5436.252814699976\n",
      "epoch=00109: log_p_X_lambda=-5436.206966522432\n",
      "epoch=00110: log_p_X_lambda=-5436.159499784964\n",
      "epoch=00111: log_p_X_lambda=-5436.110277650797\n",
      "epoch=00112: log_p_X_lambda=-5436.059150481053\n",
      "epoch=00113: log_p_X_lambda=-5436.005954724739\n",
      "epoch=00114: log_p_X_lambda=-5435.950511695137\n",
      "epoch=00115: log_p_X_lambda=-5435.8926262213445\n",
      "epoch=00116: log_p_X_lambda=-5435.832085160989\n",
      "epoch=00117: log_p_X_lambda=-5435.768655762049\n",
      "epoch=00118: log_p_X_lambda=-5435.702083858444\n",
      "epoch=00119: log_p_X_lambda=-5435.6320918863385\n",
      "epoch=00120: log_p_X_lambda=-5435.558376706571\n",
      "epoch=00121: log_p_X_lambda=-5435.480607220104\n",
      "epoch=00122: log_p_X_lambda=-5435.398421764876\n",
      "epoch=00123: log_p_X_lambda=-5435.311425284794\n",
      "epoch=00124: log_p_X_lambda=-5435.219186265227\n",
      "epoch=00125: log_p_X_lambda=-5435.121233435943\n",
      "epoch=00126: log_p_X_lambda=-5435.017052249186\n",
      "epoch=00127: log_p_X_lambda=-5434.906081153452\n",
      "epoch=00128: log_p_X_lambda=-5434.787707697638\n",
      "epoch=00129: log_p_X_lambda=-5434.6612645212\n",
      "epoch=00130: log_p_X_lambda=-5434.526025312979\n",
      "epoch=00131: log_p_X_lambda=-5434.381200856127\n",
      "epoch=00132: log_p_X_lambda=-5434.22593532262\n",
      "epoch=00133: log_p_X_lambda=-5434.0593030362625\n",
      "epoch=00134: log_p_X_lambda=-5433.880305997085\n",
      "epoch=00135: log_p_X_lambda=-5433.68787254575\n",
      "epoch=00136: log_p_X_lambda=-5433.480857654904\n",
      "epoch=00137: log_p_X_lambda=-5433.2580454601975\n",
      "epoch=00138: log_p_X_lambda=-5433.018154788523\n",
      "epoch=00139: log_p_X_lambda=-5432.759848604976\n",
      "epoch=00140: log_p_X_lambda=-5432.481748468536\n",
      "epoch=00141: log_p_X_lambda=-5432.18245525698\n",
      "epoch=00142: log_p_X_lambda=-5431.860577562678\n",
      "epoch=00143: log_p_X_lambda=-5431.514769246629\n",
      "epoch=00144: log_p_X_lambda=-5431.1437776201365\n",
      "epoch=00145: log_p_X_lambda=-5430.746503539253\n",
      "epoch=00146: log_p_X_lambda=-5430.322074269638\n",
      "epoch=00147: log_p_X_lambda=-5429.869929224353\n",
      "epoch=00148: log_p_X_lambda=-5429.389917499593\n",
      "epoch=00149: log_p_X_lambda=-5428.882404476297\n",
      "epoch=00150: log_p_X_lambda=-5428.348382598806\n",
      "epoch=00151: log_p_X_lambda=-5427.7895788700935\n",
      "epoch=00152: log_p_X_lambda=-5427.2085488463545\n",
      "epoch=00153: log_p_X_lambda=-5426.608744378024\n",
      "epoch=00154: log_p_X_lambda=-5425.994540641654\n",
      "epoch=00155: log_p_X_lambda=-5425.371207886309\n",
      "epoch=00156: log_p_X_lambda=-5424.744815547456\n",
      "epoch=00157: log_p_X_lambda=-5424.122061511118\n",
      "epoch=00158: log_p_X_lambda=-5423.510027397513\n",
      "epoch=00159: log_p_X_lambda=-5422.915871068324\n",
      "epoch=00160: log_p_X_lambda=-5422.346478580338\n",
      "epoch=00161: log_p_X_lambda=-5421.80810724346\n",
      "epoch=00162: log_p_X_lambda=-5421.306056845622\n",
      "epoch=00163: log_p_X_lambda=-5420.8444055749005\n",
      "epoch=00164: log_p_X_lambda=-5420.425840071532\n",
      "epoch=00165: log_p_X_lambda=-5420.051596420106\n",
      "epoch=00166: log_p_X_lambda=-5419.721513303863\n",
      "epoch=00167: log_p_X_lambda=-5419.434183339059\n",
      "epoch=00168: log_p_X_lambda=-5419.187176932182\n",
      "epoch=00169: log_p_X_lambda=-5418.977306872004\n",
      "epoch=00170: log_p_X_lambda=-5418.800901730448\n",
      "epoch=00171: log_p_X_lambda=-5418.6540609854455\n",
      "epoch=00172: log_p_X_lambda=-5418.532872665655\n",
      "epoch=00173: log_p_X_lambda=-5418.433583120289\n",
      "epoch=00174: log_p_X_lambda=-5418.352716473284\n",
      "epoch=00175: log_p_X_lambda=-5418.2871473551495\n",
      "epoch=00176: log_p_X_lambda=-5418.234134252176\n",
      "epoch=00177: log_p_X_lambda=-5418.191322449697\n",
      "epoch=00178: log_p_X_lambda=-5418.156725565913\n",
      "epoch=00179: log_p_X_lambda=-5418.12869365327\n",
      "epoch=00180: log_p_X_lambda=-5418.1058743122385\n",
      "epoch=00181: log_p_X_lambda=-5418.087171612176\n",
      "epoch=00182: log_p_X_lambda=-5418.071706095099\n",
      "epoch=00183: log_p_X_lambda=-5418.058777878887\n",
      "epoch=00184: log_p_X_lambda=-5418.047833915469\n",
      "epoch=00185: log_p_X_lambda=-5418.038439776697\n",
      "epoch=00186: log_p_X_lambda=-5418.03025589436\n",
      "epoch=00187: log_p_X_lambda=-5418.023017916084\n",
      "epoch=00188: log_p_X_lambda=-5418.0165207050595\n",
      "epoch=00189: log_p_X_lambda=-5418.010605466576\n",
      "epoch=00190: log_p_X_lambda=-5418.005149495138\n",
      "epoch=00191: log_p_X_lambda=-5418.000058076699\n",
      "epoch=00192: log_p_X_lambda=-5417.995258137504\n",
      "epoch=00193: log_p_X_lambda=-5417.990693291811\n",
      "epoch=00194: log_p_X_lambda=-5417.986319998046\n",
      "epoch=00195: log_p_X_lambda=-5417.982104587513\n",
      "epoch=00196: log_p_X_lambda=-5417.978020974151\n",
      "epoch=00197: log_p_X_lambda=-5417.974048893953\n",
      "epoch=00198: log_p_X_lambda=-5417.970172553743\n",
      "epoch=00199: log_p_X_lambda=-5417.966379595769\n",
      "epoch=00200: log_p_X_lambda=-5417.962660304716\n",
      "epoch=00201: log_p_X_lambda=-5417.959007000736\n",
      "epoch=00202: log_p_X_lambda=-5417.955413574804\n",
      "epoch=00203: log_p_X_lambda=-5417.951875132983\n",
      "epoch=00204: log_p_X_lambda=-5417.948387723547\n",
      "epoch=00205: log_p_X_lambda=-5417.944948127876\n",
      "epoch=00206: log_p_X_lambda=-5417.941553699457\n",
      "epoch=00207: log_p_X_lambda=-5417.938202240202\n",
      "epoch=00208: log_p_X_lambda=-5417.934891904777\n",
      "epoch=00209: log_p_X_lambda=-5417.931621126754\n",
      "epoch=00210: log_p_X_lambda=-5417.928388561202\n",
      "epoch=00211: log_p_X_lambda=-5417.925193040116\n",
      "epoch=00212: log_p_X_lambda=-5417.922033537574\n",
      "epoch=00213: log_p_X_lambda=-5417.918909142389\n",
      "epoch=00214: log_p_X_lambda=-5417.915819036653\n",
      "epoch=00215: log_p_X_lambda=-5417.912762478703\n",
      "epoch=00216: log_p_X_lambda=-5417.909738789795\n",
      "epoch=00217: log_p_X_lambda=-5417.906747343219\n",
      "epoch=00218: log_p_X_lambda=-5417.903787555818\n",
      "epoch=00219: log_p_X_lambda=-5417.900858880966\n",
      "epoch=00220: log_p_X_lambda=-5417.897960803166\n",
      "epoch=00221: log_p_X_lambda=-5417.895092833334\n",
      "epoch=00222: log_p_X_lambda=-5417.892254505189\n",
      "epoch=00223: log_p_X_lambda=-5417.889445372217\n",
      "epoch=00224: log_p_X_lambda=-5417.886665005173\n",
      "epoch=00225: log_p_X_lambda=-5417.883912990037\n",
      "epoch=00226: log_p_X_lambda=-5417.881188926226\n",
      "epoch=00227: log_p_X_lambda=-5417.8784924253\n",
      "epoch=00228: log_p_X_lambda=-5417.875823109661\n",
      "epoch=00229: log_p_X_lambda=-5417.873180611476\n",
      "epoch=00230: log_p_X_lambda=-5417.870564572009\n",
      "epoch=00231: log_p_X_lambda=-5417.867974640794\n",
      "epoch=00232: log_p_X_lambda=-5417.86541047498\n",
      "epoch=00233: log_p_X_lambda=-5417.862871738868\n",
      "epoch=00234: log_p_X_lambda=-5417.860358103395\n",
      "epoch=00235: log_p_X_lambda=-5417.857869245859\n",
      "epoch=00236: log_p_X_lambda=-5417.855404849413\n",
      "epoch=00237: log_p_X_lambda=-5417.852964602869\n",
      "epoch=00238: log_p_X_lambda=-5417.850548200442\n",
      "epoch=00239: log_p_X_lambda=-5417.848155341468\n",
      "epoch=00240: log_p_X_lambda=-5417.845785730211\n",
      "epoch=00241: log_p_X_lambda=-5417.843439075788\n",
      "epoch=00242: log_p_X_lambda=-5417.841115091707\n",
      "epoch=00243: log_p_X_lambda=-5417.838813496134\n",
      "epoch=00244: log_p_X_lambda=-5417.836534011462\n",
      "epoch=00245: log_p_X_lambda=-5417.834276364307\n",
      "epoch=00246: log_p_X_lambda=-5417.8320402853005\n",
      "epoch=00247: log_p_X_lambda=-5417.82982550913\n",
      "epoch=00248: log_p_X_lambda=-5417.827631774364\n",
      "epoch=00249: log_p_X_lambda=-5417.82545882333\n",
      "epoch=00250: log_p_X_lambda=-5417.823306402125\n",
      "epoch=00251: log_p_X_lambda=-5417.821174260465\n",
      "epoch=00252: log_p_X_lambda=-5417.819062151634\n",
      "epoch=00253: log_p_X_lambda=-5417.816969832379\n",
      "epoch=00254: log_p_X_lambda=-5417.814897062988\n",
      "epoch=00255: log_p_X_lambda=-5417.812843607016\n",
      "epoch=00256: log_p_X_lambda=-5417.810809231374\n",
      "epoch=00257: log_p_X_lambda=-5417.808793706244\n",
      "epoch=00258: log_p_X_lambda=-5417.806796804973\n",
      "epoch=00259: log_p_X_lambda=-5417.804818304115\n",
      "epoch=00260: log_p_X_lambda=-5417.802857983204\n",
      "epoch=00261: log_p_X_lambda=-5417.800915624943\n",
      "epoch=00262: log_p_X_lambda=-5417.798991014965\n",
      "epoch=00263: log_p_X_lambda=-5417.7970839418695\n",
      "epoch=00264: log_p_X_lambda=-5417.795194197186\n",
      "epoch=00265: log_p_X_lambda=-5417.793321575257\n",
      "epoch=00266: log_p_X_lambda=-5417.7914658732125\n",
      "epoch=00267: log_p_X_lambda=-5417.78962689105\n",
      "epoch=00268: log_p_X_lambda=-5417.787804431422\n",
      "epoch=00269: log_p_X_lambda=-5417.785998299684\n",
      "epoch=00270: log_p_X_lambda=-5417.784208303825\n",
      "epoch=00271: log_p_X_lambda=-5417.7824342544645\n",
      "epoch=00272: log_p_X_lambda=-5417.780675964756\n",
      "epoch=00273: log_p_X_lambda=-5417.778933250436\n",
      "epoch=00274: log_p_X_lambda=-5417.777205929678\n",
      "epoch=00275: log_p_X_lambda=-5417.775493823111\n",
      "epoch=00276: log_p_X_lambda=-5417.773796753825\n",
      "epoch=00277: log_p_X_lambda=-5417.772114547251\n",
      "epoch=00278: log_p_X_lambda=-5417.770447031164\n",
      "epoch=00279: log_p_X_lambda=-5417.768794035693\n",
      "epoch=00280: log_p_X_lambda=-5417.767155393189\n",
      "epoch=00281: log_p_X_lambda=-5417.765530938282\n",
      "epoch=00282: log_p_X_lambda=-5417.763920507796\n",
      "epoch=00283: log_p_X_lambda=-5417.762323940773\n",
      "epoch=00284: log_p_X_lambda=-5417.760741078333\n",
      "epoch=00285: log_p_X_lambda=-5417.75917176375\n",
      "epoch=00286: log_p_X_lambda=-5417.757615842389\n",
      "epoch=00287: log_p_X_lambda=-5417.75607316166\n",
      "epoch=00288: log_p_X_lambda=-5417.754543570936\n",
      "epoch=00289: log_p_X_lambda=-5417.753026921688\n",
      "epoch=00290: log_p_X_lambda=-5417.75152306734\n",
      "epoch=00291: log_p_X_lambda=-5417.750031863131\n",
      "epoch=00292: log_p_X_lambda=-5417.7485531663315\n",
      "epoch=00293: log_p_X_lambda=-5417.747086836072\n",
      "epoch=00294: log_p_X_lambda=-5417.745632733266\n",
      "epoch=00295: log_p_X_lambda=-5417.744190720716\n",
      "epoch=00296: log_p_X_lambda=-5417.742760663041\n",
      "epoch=00297: log_p_X_lambda=-5417.741342426552\n",
      "epoch=00298: log_p_X_lambda=-5417.739935879361\n",
      "epoch=00299: log_p_X_lambda=-5417.738540891329\n",
      "epoch=00300: log_p_X_lambda=-5417.737157333939\n",
      "epoch=00301: log_p_X_lambda=-5417.735785080415\n",
      "epoch=00302: log_p_X_lambda=-5417.734424005577\n",
      "epoch=00303: log_p_X_lambda=-5417.7330739859135\n",
      "epoch=00304: log_p_X_lambda=-5417.731734899475\n",
      "epoch=00305: log_p_X_lambda=-5417.730406625915\n",
      "epoch=00306: log_p_X_lambda=-5417.729089046444\n",
      "epoch=00307: log_p_X_lambda=-5417.727782043781\n",
      "epoch=00308: log_p_X_lambda=-5417.726485502191\n",
      "epoch=00309: log_p_X_lambda=-5417.725199307366\n",
      "epoch=00310: log_p_X_lambda=-5417.723923346551\n",
      "epoch=00311: log_p_X_lambda=-5417.722657508395\n",
      "epoch=00312: log_p_X_lambda=-5417.721401682922\n",
      "epoch=00313: log_p_X_lambda=-5417.720155761643\n",
      "epoch=00314: log_p_X_lambda=-5417.718919637425\n",
      "epoch=00315: log_p_X_lambda=-5417.717693204511\n",
      "epoch=00316: log_p_X_lambda=-5417.716476358432\n",
      "epoch=00317: log_p_X_lambda=-5417.71526899614\n",
      "epoch=00318: log_p_X_lambda=-5417.71407101582\n",
      "epoch=00319: log_p_X_lambda=-5417.712882316995\n",
      "epoch=00320: log_p_X_lambda=-5417.7117028003595\n",
      "epoch=00321: log_p_X_lambda=-5417.710532368032\n",
      "epoch=00322: log_p_X_lambda=-5417.7093709232395\n",
      "epoch=00323: log_p_X_lambda=-5417.708218370445\n",
      "epoch=00324: log_p_X_lambda=-5417.707074615363\n",
      "epoch=00325: log_p_X_lambda=-5417.7059395648075\n",
      "epoch=00326: log_p_X_lambda=-5417.704813126857\n",
      "epoch=00327: log_p_X_lambda=-5417.703695210628\n",
      "epoch=00328: log_p_X_lambda=-5417.7025857265\n",
      "epoch=00329: log_p_X_lambda=-5417.7014845858575\n",
      "epoch=00330: log_p_X_lambda=-5417.700391701275\n",
      "epoch=00331: log_p_X_lambda=-5417.699306986346\n",
      "epoch=00332: log_p_X_lambda=-5417.698230355791\n",
      "epoch=00333: log_p_X_lambda=-5417.69716172536\n",
      "epoch=00334: log_p_X_lambda=-5417.696101011779\n",
      "epoch=00335: log_p_X_lambda=-5417.695048132954\n",
      "epoch=00336: log_p_X_lambda=-5417.694003007684\n",
      "epoch=00337: log_p_X_lambda=-5417.692965555827\n",
      "epoch=00338: log_p_X_lambda=-5417.69193569819\n",
      "epoch=00339: log_p_X_lambda=-5417.690913356508\n",
      "epoch=00340: log_p_X_lambda=-5417.689898453617\n",
      "epoch=00341: log_p_X_lambda=-5417.688890913183\n",
      "epoch=00342: log_p_X_lambda=-5417.687890659808\n",
      "epoch=00343: log_p_X_lambda=-5417.686897619067\n",
      "epoch=00344: log_p_X_lambda=-5417.6859117173935\n",
      "epoch=00345: log_p_X_lambda=-5417.684932882139\n",
      "epoch=00346: log_p_X_lambda=-5417.683961041551\n",
      "epoch=00347: log_p_X_lambda=-5417.6829961246785\n",
      "epoch=00348: log_p_X_lambda=-5417.682038061516\n",
      "epoch=00349: log_p_X_lambda=-5417.6810867828635\n",
      "epoch=00350: log_p_X_lambda=-5417.68014222032\n",
      "epoch=00351: log_p_X_lambda=-5417.679204306367\n",
      "epoch=00352: log_p_X_lambda=-5417.678272974297\n",
      "epoch=00353: log_p_X_lambda=-5417.67734815812\n",
      "epoch=00354: log_p_X_lambda=-5417.676429792714\n",
      "epoch=00355: log_p_X_lambda=-5417.675517813758\n",
      "epoch=00356: log_p_X_lambda=-5417.674612157596\n",
      "epoch=00357: log_p_X_lambda=-5417.673712761432\n",
      "epoch=00358: log_p_X_lambda=-5417.6728195631695\n",
      "epoch=00359: log_p_X_lambda=-5417.671932501461\n",
      "epoch=00360: log_p_X_lambda=-5417.67105151565\n",
      "epoch=00361: log_p_X_lambda=-5417.670176545875\n",
      "epoch=00362: log_p_X_lambda=-5417.669307532865\n",
      "epoch=00363: log_p_X_lambda=-5417.668444418196\n",
      "epoch=00364: log_p_X_lambda=-5417.667587144029\n",
      "epoch=00365: log_p_X_lambda=-5417.666735653214\n",
      "epoch=00366: log_p_X_lambda=-5417.6658898892965\n",
      "epoch=00367: log_p_X_lambda=-5417.66504979648\n",
      "epoch=00368: log_p_X_lambda=-5417.664215319641\n",
      "epoch=00369: log_p_X_lambda=-5417.663386404231\n",
      "epoch=00370: log_p_X_lambda=-5417.6625629964\n",
      "epoch=00371: log_p_X_lambda=-5417.661745042941\n",
      "epoch=00372: log_p_X_lambda=-5417.660932491195\n",
      "epoch=00373: log_p_X_lambda=-5417.66012528916\n",
      "epoch=00374: log_p_X_lambda=-5417.659323385421\n",
      "epoch=00375: log_p_X_lambda=-5417.658526729147\n",
      "epoch=00376: log_p_X_lambda=-5417.657735270153\n",
      "epoch=00377: log_p_X_lambda=-5417.656948958773\n",
      "epoch=00378: log_p_X_lambda=-5417.656167745932\n",
      "epoch=00379: log_p_X_lambda=-5417.655391583088\n",
      "epoch=00380: log_p_X_lambda=-5417.654620422326\n",
      "epoch=00381: log_p_X_lambda=-5417.653854216211\n",
      "epoch=00382: log_p_X_lambda=-5417.653092917873\n",
      "epoch=00383: log_p_X_lambda=-5417.652336481007\n",
      "epoch=00384: log_p_X_lambda=-5417.651584859768\n",
      "epoch=00385: log_p_X_lambda=-5417.65083800891\n",
      "epoch=00386: log_p_X_lambda=-5417.6500958836305\n",
      "epoch=00387: log_p_X_lambda=-5417.649358439716\n",
      "epoch=00388: log_p_X_lambda=-5417.64862563333\n",
      "epoch=00389: log_p_X_lambda=-5417.647897421273\n",
      "epoch=00390: log_p_X_lambda=-5417.647173760754\n",
      "epoch=00391: log_p_X_lambda=-5417.646454609458\n",
      "epoch=00392: log_p_X_lambda=-5417.645739925598\n",
      "epoch=00393: log_p_X_lambda=-5417.64502966775\n",
      "epoch=00394: log_p_X_lambda=-5417.644323795074\n",
      "epoch=00395: log_p_X_lambda=-5417.643622267132\n",
      "epoch=00396: log_p_X_lambda=-5417.642925043984\n",
      "epoch=00397: log_p_X_lambda=-5417.642232086007\n",
      "epoch=00398: log_p_X_lambda=-5417.641543354143\n",
      "epoch=00399: log_p_X_lambda=-5417.640858809786\n",
      "epoch=00400: log_p_X_lambda=-5417.6401784146465\n",
      "epoch=00401: log_p_X_lambda=-5417.639502130915\n",
      "epoch=00402: log_p_X_lambda=-5417.638829921214\n",
      "epoch=00403: log_p_X_lambda=-5417.638161748573\n",
      "epoch=00404: log_p_X_lambda=-5417.637497576365\n",
      "epoch=00405: log_p_X_lambda=-5417.63683736854\n",
      "epoch=00406: log_p_X_lambda=-5417.636181089207\n",
      "epoch=00407: log_p_X_lambda=-5417.635528703058\n",
      "epoch=00408: log_p_X_lambda=-5417.634880175062\n",
      "epoch=00409: log_p_X_lambda=-5417.634235470632\n",
      "epoch=00410: log_p_X_lambda=-5417.6335945554965\n",
      "epoch=00411: log_p_X_lambda=-5417.6329573958465\n",
      "epoch=00412: log_p_X_lambda=-5417.632323958142\n",
      "epoch=00413: log_p_X_lambda=-5417.6316942093\n",
      "epoch=00414: log_p_X_lambda=-5417.631068116516\n",
      "epoch=00415: log_p_X_lambda=-5417.630445647415\n",
      "epoch=00416: log_p_X_lambda=-5417.6298267698485\n",
      "epoch=00417: log_p_X_lambda=-5417.629211452192\n",
      "epoch=00418: log_p_X_lambda=-5417.628599663026\n",
      "epoch=00419: log_p_X_lambda=-5417.627991371316\n",
      "epoch=00420: log_p_X_lambda=-5417.6273865463145\n",
      "epoch=00421: log_p_X_lambda=-5417.626785157694\n",
      "epoch=00422: log_p_X_lambda=-5417.62618717539\n",
      "epoch=00423: log_p_X_lambda=-5417.625592569653\n",
      "epoch=00424: log_p_X_lambda=-5417.625001311062\n",
      "epoch=00425: log_p_X_lambda=-5417.624413370536\n",
      "epoch=00426: log_p_X_lambda=-5417.623828719283\n",
      "epoch=00427: log_p_X_lambda=-5417.623247328744\n",
      "epoch=00428: log_p_X_lambda=-5417.622669170822\n",
      "epoch=00429: log_p_X_lambda=-5417.622094217577\n",
      "epoch=00430: log_p_X_lambda=-5417.621522441412\n",
      "epoch=00431: log_p_X_lambda=-5417.62095381506\n",
      "epoch=00432: log_p_X_lambda=-5417.620388311434\n",
      "epoch=00433: log_p_X_lambda=-5417.6198259038965\n",
      "epoch=00434: log_p_X_lambda=-5417.619266565891\n",
      "epoch=00435: log_p_X_lambda=-5417.618710271317\n",
      "epoch=00436: log_p_X_lambda=-5417.618156994206\n",
      "epoch=00437: log_p_X_lambda=-5417.617606708962\n",
      "epoch=00438: log_p_X_lambda=-5417.617059390213\n",
      "epoch=00439: log_p_X_lambda=-5417.6165150128745\n",
      "epoch=00440: log_p_X_lambda=-5417.61597355206\n",
      "epoch=00441: log_p_X_lambda=-5417.615434983198\n",
      "epoch=00442: log_p_X_lambda=-5417.6148992820035\n",
      "epoch=00443: log_p_X_lambda=-5417.614366424345\n",
      "epoch=00444: log_p_X_lambda=-5417.61383638637\n",
      "epoch=00445: log_p_X_lambda=-5417.613309144535\n",
      "epoch=00446: log_p_X_lambda=-5417.6127846755135\n",
      "epoch=00447: log_p_X_lambda=-5417.612262956172\n",
      "epoch=00448: log_p_X_lambda=-5417.611743963628\n",
      "epoch=00449: log_p_X_lambda=-5417.6112276752565\n",
      "epoch=00450: log_p_X_lambda=-5417.610714068666\n",
      "epoch=00451: log_p_X_lambda=-5417.610203121691\n",
      "epoch=00452: log_p_X_lambda=-5417.609694812369\n",
      "epoch=00453: log_p_X_lambda=-5417.609189118955\n",
      "epoch=00454: log_p_X_lambda=-5417.608686019952\n",
      "epoch=00455: log_p_X_lambda=-5417.6081854941085\n",
      "epoch=00456: log_p_X_lambda=-5417.607687520298\n",
      "epoch=00457: log_p_X_lambda=-5417.607192077703\n",
      "epoch=00458: log_p_X_lambda=-5417.606699145654\n",
      "epoch=00459: log_p_X_lambda=-5417.606208703719\n",
      "epoch=00460: log_p_X_lambda=-5417.605720731664\n",
      "epoch=00461: log_p_X_lambda=-5417.605235209427\n",
      "epoch=00462: log_p_X_lambda=-5417.604752117198\n",
      "epoch=00463: log_p_X_lambda=-5417.604271435319\n",
      "epoch=00464: log_p_X_lambda=-5417.603793144406\n",
      "epoch=00465: log_p_X_lambda=-5417.603317225164\n",
      "epoch=00466: log_p_X_lambda=-5417.602843658537\n",
      "epoch=00467: log_p_X_lambda=-5417.602372425725\n",
      "epoch=00468: log_p_X_lambda=-5417.60190350795\n",
      "epoch=00469: log_p_X_lambda=-5417.601436886788\n",
      "epoch=00470: log_p_X_lambda=-5417.600972543912\n",
      "epoch=00471: log_p_X_lambda=-5417.600510461202\n",
      "epoch=00472: log_p_X_lambda=-5417.600050620704\n",
      "epoch=00473: log_p_X_lambda=-5417.599593004624\n",
      "epoch=00474: log_p_X_lambda=-5417.599137595387\n",
      "epoch=00475: log_p_X_lambda=-5417.598684375559\n",
      "epoch=00476: log_p_X_lambda=-5417.598233327876\n",
      "epoch=00477: log_p_X_lambda=-5417.597784435287\n",
      "epoch=00478: log_p_X_lambda=-5417.597337680838\n",
      "epoch=00479: log_p_X_lambda=-5417.596893047768\n",
      "epoch=00480: log_p_X_lambda=-5417.59645051949\n",
      "epoch=00481: log_p_X_lambda=-5417.596010079587\n",
      "epoch=00482: log_p_X_lambda=-5417.595571711772\n",
      "epoch=00483: log_p_X_lambda=-5417.595135399952\n",
      "epoch=00484: log_p_X_lambda=-5417.59470112816\n",
      "epoch=00485: log_p_X_lambda=-5417.594268880581\n",
      "epoch=00486: log_p_X_lambda=-5417.593838641557\n",
      "epoch=00487: log_p_X_lambda=-5417.593410395596\n",
      "epoch=00488: log_p_X_lambda=-5417.592984127341\n",
      "epoch=00489: log_p_X_lambda=-5417.592559821598\n",
      "epoch=00490: log_p_X_lambda=-5417.592137463311\n",
      "epoch=00491: log_p_X_lambda=-5417.591717037542\n",
      "epoch=00492: log_p_X_lambda=-5417.591298529541\n",
      "epoch=00493: log_p_X_lambda=-5417.590881924652\n",
      "epoch=00494: log_p_X_lambda=-5417.590467208416\n",
      "epoch=00495: log_p_X_lambda=-5417.590054366416\n",
      "epoch=00496: log_p_X_lambda=-5417.589643384525\n",
      "epoch=00497: log_p_X_lambda=-5417.589234248565\n",
      "epoch=00498: log_p_X_lambda=-5417.588826944668\n",
      "epoch=00499: log_p_X_lambda=-5417.58842145894\n"
     ]
    }
   ],
   "source": [
    "# running Baum-Welch\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    _, log_alpha, log_beta, log_p_x_lambda = e_step(x=x, log_pi=log_pi_est, log_a=log_a_est, log_b=log_b_est)\n",
    "    log_pi_est, log_a_est, log_b_est = m_step(x=x, x_one_hot=x_one_hot, log_a=log_a_est, log_b=log_b_est, log_alpha=log_alpha, log_beta=log_beta, log_p_x_lambda=log_p_x_lambda)\n",
    "    log_p_X_lambda = log_p_x_lambda[:, 0].sum()\n",
    "    print(f\"{epoch=:05}: {log_p_X_lambda=}\")\n",
    "\n",
    "    # Note: log_p_X_lambda (the log-likelihood) should always increase, we could test for this and use the difference in log-likelihoods as a stopping criteria ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.exp(log_pi_est)=array([0.41666108, 0.58333892])\n",
      "np.exp(log_a_est)=array([[0.63155233, 0.36844767],\n",
      "       [0.32702993, 0.67297007]])\n",
      "np.exp(log_b_est)=array([[0.11315723, 0.40124073, 0.48560203],\n",
      "       [0.62583838, 0.27979874, 0.09436288]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.exp(log_pi_est)=}\")\n",
    "print(f\"{np.exp(log_a_est)=}\")\n",
    "print(f\"{np.exp(log_b_est)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the algorithm outputs valid parameters for our HMM (i.e. respecting their summation constraints)\n",
    "\n",
    "assert np.allclose(np.exp(log_pi_est).sum(), 1)\n",
    "assert np.allclose(np.exp(log_a_est).sum(axis=-1), 1)\n",
    "assert np.allclose(np.exp(log_b_est).sum(axis=-1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying over Viterbi from Exercise 2\n",
    "\n",
    "def viterbi_v(x, pi, a, b):\n",
    "    num_states = pi.size\n",
    "    seq_len = len(x)\n",
    "\n",
    "    # let's work on the log domain (base 2)\n",
    "    log_pi = np.log2(pi)\n",
    "    log_a = np.log2(a)\n",
    "    log_b = np.log2(b)\n",
    "\n",
    "    log_delta = np.zeros((seq_len, num_states))\n",
    "    psi = np.full((seq_len, num_states), -1, dtype=np.int32)\n",
    "\n",
    "    # initilization\n",
    "    log_delta[0] = log_pi + log_b[:, x[0]]\n",
    "\n",
    "    for t in range(1, seq_len):\n",
    "        # Sum log_delta[t-1] (array of log_delta in the previous step) with\n",
    "        # log_a matrix. Since delta[t-1] is an array, we want to broadcast its\n",
    "        # second dimension through the first dimension of log_a (which\n",
    "        # represents the i state on a transition i->j). So that, in the end,\n",
    "        # we have log_delta_a[i,j] = log_delta[t-1,i] + log_a[i,j]\n",
    "        log_delta_a = log_delta[t - 1].reshape(-1, 1) + log_a\n",
    "\n",
    "        # log_delta is the max value plus the emission probability on\n",
    "        # time-step t. The max is taken along the first dimension, i.e., the max\n",
    "        # along the previous state `i` for each state `j`.\n",
    "        log_delta[t] = log_delta_a.max(axis=0) + log_b[:, x[t]]\n",
    "\n",
    "        # psi is the argmax\n",
    "        psi[t] = log_delta_a.argmax(axis=0)\n",
    "\n",
    "    # now we recover the most likely sequence of states backwards, from the last\n",
    "    # time step (in Python, -1 represents the last index of a list/array) up to\n",
    "    # the first one.\n",
    "    y = []\n",
    "    # here we use the argmax() method of a numpy array to find the best\n",
    "    # state on the last time step\n",
    "    y.append(log_delta[-1].argmax())\n",
    "    for t in range(seq_len - 1, 0, -1):\n",
    "        y.append(psi[t, y[-1]])\n",
    "\n",
    "    # the list y is backwards, so reverse it and return\n",
    "    y.reverse()\n",
    "    return y, log_delta, psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "example = np.array([WALK, SHOP, CLEAN, SHOP, WALK])\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _, _ = viterbi_v(x=example, pi=np.exp(log_pi_est), a=np.exp(log_a_est), b=np.exp(log_b_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation: in the run above, states are reversed from exercise 1, suggesting, that state 1 might correspond to \"Sunny\" and state 0 might correspond to \"Rainy\"\n",
    "# (this might be different for your run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with ground truth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth copied from exercise 1\n",
    "\n",
    "pi = np.array([0.6, 0.4])\n",
    "\n",
    "a = np.array(\n",
    "    [\n",
    "        [0.7, 0.3],  # transitions a_0j (from state 0 (Sunny) to some state j)\n",
    "        [0.4, 0.6],  # transitions a_1j (from state 1 (Rainy) to some state j)\n",
    "    ]\n",
    ")\n",
    "\n",
    "b = np.array(\n",
    "    [\n",
    "        [0.6, 0.3, 0.1],  # emissions b_0(k) (in state 0 (Sunny) emits k)\n",
    "        [0.1, 0.4, 0.5],  # emissions b_1(k) (in state 1 (Rainy) emits k)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.exp(log_pi_est)=array([0.41666108, 0.58333892])\n",
      "np.exp(log_a_est)=array([[0.63155233, 0.36844767],\n",
      "       [0.32702993, 0.67297007]])\n",
      "np.exp(log_b_est)=array([[0.11315723, 0.40124073, 0.48560203],\n",
      "       [0.62583838, 0.27979874, 0.09436288]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.exp(log_pi_est)=}\")\n",
    "print(f\"{np.exp(log_a_est)=}\")\n",
    "print(f\"{np.exp(log_b_est)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation: indeed, if one changes the ordering of states the match looks pretty decent ... (again, this depends on the run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
