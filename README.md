# Forecasting and Simulation

Theories and practical applications of statistical and machine learning methods for predicting and simulating temporal data, including time-series forecasting and data modeling.


Predicting and Simulating Temporal Data

This repository contains a collection of Jupyter notebooks focused on applying statistical and machine learning methods to data with temporal dependencies. The examples demonstrate both theoretical concepts and practical approaches for analyzing time-dependent data.



Topics Covered:
1. HMM: (slides, rabiner, Stanford x2)
    a. Forward-backward (ex8 t3, ex9 t2)
    b. Viterbi
    c. EM (learning)
    d. Baum-Welch (ex8 t1&2, ex9 t1, bishop)
2. Log-sum-exp trick, exercise3 task2 (Hidden Markov perceptron learning)
3. Exercise 4, task 1 & 2 & 3 (very important)(contain examples of primal and dual perceptron):
    a. Primal and dual perceptron (slides, altun, notes)
    b. Scoring function (dual perceptron)
4. CRF, MRF: (ex6)
    a. Partition function (ex5) & potential function
    b. Feature Template (ex5)
    c. Structure perceptron for CRF (ex5)
    d. MAP estimation vs Posterior (slides)
(Check for more detailed explanations on this [medium article](https://medium.com/data-science-in-your-pocket/named-entity-recognition-ner-using-conditional-random-fields-in-nlp-3660df22e95c)
5. Expectation Maximization: (ex6, ex7)
    a. Mixtures of Bernoulli distributions (bishop, 9.3.3)
    b. Jenssenâ€™s inequality
    c. Kullback-leibler divergence (bishop? Or ex7 t2)
6. HM-SVM: (altun, collins, SVM stanford)
    a. Working set optimization (ex9 t3)
    b. Dual of soft margin HM-SVM (ex9 t3)
    c. Penalty term
7. Other topics in previous years exams and covered in the slide:
    a. Kernel trick
    b. Empirical Risk Minimization
    c. KKT conditions
    d. Hinge loss
    e. Slack variable & rescaling
    f. K-means
    g. Lagrangian multipliers
    h. Kernels, RBF kernels, polynomial kernels
    i. Regularization Theory
    j. Representer Theorem

    
